# -*- coding: utf-8 -*-
"""yelp_review_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12JirFBhw-KlGyYqadBftyhbpP3kl3MMr
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords

# Importing and Reading the review metatrain dataset 
df = pd.read_csv('/content/1861059754_reviewmetatrain.csv')
# printing three rows of the dataset
df.head(3)

# printing the shape of the dataset
df.shape

# checking for more information about the dataset
df.info()

# To get insights and statisics of the dataset
df.describe()

# reading the text review content
text = pd.read_csv('/content/1102865911_reviewtexttrain.csv')

# creating a new column  called review length that the length of ecah review
df['review length'] = text['review'].apply(len)
# printing the new dataframe after adding a new column
df.head()

# trying to visualize more about the dataset by plotting graphs using seaborn
graph = sns.FacetGrid(data=df, col='rating')
graph.map(plt.hist, 'review length', bins=50)

# creating a box plot of the review length for each rating
sns.boxplot(x='rating', y='review length', data=df)

# Grouping the data by rating and getting the correration
rating = df.groupby('rating').mean()
rating.corr()

# Visualizing the correration
sns.heatmap(data=rating.corr(), annot=True)

# grabbing reviews that are either 1 or 5 stars from the yelp dataframe
df_class = df[(df['rating'] == 1) | (df['rating'] == 5)]
# printing the shape of the new dataframe
df_class.shape

# x is the review column of text
x = text['review']
# y will be the rating column.
y = df["rating"]

# printing a sample review
x[0]

# text pre-processing
import string
def process(text_data):

  '''
    Takes in a string of text, then performs the following:
    Remove all punctuation
    Remove all stopwords
    Return the cleaned text as a list of words
  '''
  no_punctuations = [char for char in text_data if char not in string.punctuation]
  no_punctuations = ''.join(no_punctuations)
    
  return [word for word in no_punctuations.split() if word.lower() not in stopwords.words('english')]

# This is a fix to the error that is caused when trying to download stopwords from ntlk
nltk.download('stopwords')

# Vectorization
# importing CountVectorizer from sklearn
# I used Scikit-learnâ€™s CountVectorizer to convert the text collection into a matrix of token counts
from sklearn.feature_extraction.text import CountVectorizer

bow_transformer = CountVectorizer(analyzer=process).fit(x)

# initializing CountVectorizer
cv = CountVectorizer()

# transforming x from text to vector
X = bow_transformer.transform(x)

X

# importing train_test_split from skleran
from sklearn.model_selection import train_test_split

# split the adata X and y into a training and a test set 
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)

# To load the matrix
pd.read_csv(r"/content/review_text_train_doc2vec50.csv", index_col = False, delimiter = ',', header=None)

pd.read_csv(r"/content/review_text_test_doc2vec100.csv", index_col = False, delimiter = ',', header=None)

import pandas as pd
pd.read_csv(r"/content/review_text_test_doc2vec50.csv", index_col = False, delimiter = ',', header=None)

import pandas as pd
pd.read_csv(r"/content/review_text_train_doc2vec100.csv", index_col = False, delimiter = ',', header=None)

# To load the sparse matrix:
import scipy
x_train = scipy.sparse.load_npz('/content/review_text_train_vec.npz')
x_train

# To load the sparse matrix:
import scipy
x_test = scipy.sparse.load_npz('/content/review_text_test_vec.npz')
x_test

import pickle
# getting the vocabulary dictionary
vocab = pickle.load(open("/content/train_countvectorizer.pkl", "rb"))
vocab_dict = vocab.vocabulary_
# len(vocab_dict)
vocab_dict

# Training The Model
# importing MultinomialNB from sklearn
# Multinomial Naive Bayes is a specialised version of Naive Bayes designed more for text documents
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

# Fitting the training dataset 
nb.fit(X_train,y_train)

# Using our trained classifier to predict the ratings from text
# Testing our model on the test set

preds = nb.predict(X_test)
print("Actual Ratings(rating): ",end = "")
display(y_test[:15])
print("Predicted Ratings: ",end = "")
print(preds[:15])

import pickle
f = open('my_classifier.pickle', 'wb')
pickle.dump(nb, f)
f.close()

# importing classification report and confussion matrix
from sklearn.metrics import confusion_matrix,classification_report

print(confusion_matrix(y_test,predictions))
print('\n')
print(classification_report(y_test,predictions))

from sklearn.feature_extraction.text import  TfidfTransformer
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('bow', CountVectorizer()),  # strings to token integer counts
    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier
])

X = text['review']
y = df['rating']
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)

pipeline.fit(X_train,y_train)

predictions = pipeline.predict(X_test)
print("Actual Ratings(rating): ",end = "")
display(y_test[:15])
print("Predicted Ratings: ",end = "")
# print(predictions[:15])

######### if you need another copy of the predicted output please uncomment this cell and run it ###

# import pickle
# f = open('my_classifier.pickle', 'wb')
# pickle.dump(nb, f)
# f.close()

print(confusion_matrix(y_test,predictions))
print(classification_report(y_test,predictions))